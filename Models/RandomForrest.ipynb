{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, make_scorer, mean_absolute_percentage_error\n",
    "\n",
    "# Expand display options\n",
    "pd.set_option(\"display.max_rows\", None)  # Show all rows\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Do not truncate column text\n",
    "pd.set_option(\"display.expand_frame_repr\", False)  # Avoid line wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OHE = pd.read_csv('../DataSet/RegressionData/healthinsurance_OHE.csv')\n",
    "df_LE = pd.read_csv('../DataSet/RegressionData/healthinsurance_LE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Best parameters found: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': None, 'max_depth': 30, 'ccp_alpha': 0.0, 'bootstrap': False}\n",
      "Best cross-validation score: -0.036222122385397654\n",
      "Train set R² score: 1.0\n",
      "Train set MAPE: 0.00%\n",
      "Test set R² score: 0.9556124553263627\n",
      "Test set MAPE: 3.05%\n"
     ]
    }
   ],
   "source": [
    "X = df_LE.drop('claim', axis=1)\n",
    "\n",
    "y = df_LE['claim']\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model directly (no pipeline needed)\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=1)  # Enable parallel tree training\n",
    "\n",
    "# Expanded parameter grid with corrected max_features values\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400],\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "    'max_features': ['sqrt', 'log2', None, 0.5, 0.7, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.1],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.01, 0.1]  \n",
    "}\n",
    "\n",
    "mape_scorer = make_scorer(abs(mean_absolute_percentage_error), greater_is_better=False)  # Minimize error\n",
    "\n",
    "# Set up the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,       # Directly use the RandomForestRegressor\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=200,         # Number of parameter settings sampled\n",
    "    cv=5,              # 5-fold cross-validation\n",
    "    n_jobs=18,         # Use all available cores for parallel search\n",
    "    random_state=42,   # For reproducibility\n",
    "    verbose=3,         # Show intermediate progress\n",
    "    scoring=mape_scorer,\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV on the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the training set\n",
    "y_train_pred = random_search.best_estimator_.predict(X_train)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred) * 100  # Convert to\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = random_search.best_estimator_.predict(X_test)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred) * 100  # Convert to percentage\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "print(\"Train set R² score:\", r2_train)\n",
    "print(f\"Train set MAPE: {mape_train:.2f}%\")\n",
    "print(\"Test set R² score:\", r2_test)\n",
    "print(f\"Test set MAPE: {mape_test:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': None, 'max_depth': 30, 'ccp_alpha': 0.0, 'bootstrap': False}</td>\n",
       "      <td>-0.036222</td>\n",
       "      <td>0.002755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>{'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 1.0, 'max_depth': 50, 'ccp_alpha': 0.1, 'bootstrap': False}</td>\n",
       "      <td>-0.036372</td>\n",
       "      <td>0.002713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 8, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.7, 'max_depth': 40, 'ccp_alpha': 0.01, 'bootstrap': False}</td>\n",
       "      <td>-0.036404</td>\n",
       "      <td>0.002429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': None, 'max_depth': None, 'ccp_alpha': 0.01, 'bootstrap': False}</td>\n",
       "      <td>-0.036460</td>\n",
       "      <td>0.002638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>{'n_estimators': 300, 'min_samples_split': 4, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 1.0, 'max_depth': 20, 'ccp_alpha': 0.001, 'bootstrap': False}</td>\n",
       "      <td>-0.036617</td>\n",
       "      <td>0.002601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>{'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'log2', 'max_depth': None, 'ccp_alpha': 0.1, 'bootstrap': False}</td>\n",
       "      <td>-0.037666</td>\n",
       "      <td>0.001694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 35, 'ccp_alpha': 0.0, 'bootstrap': False}</td>\n",
       "      <td>-0.037889</td>\n",
       "      <td>0.001804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 'log2', 'max_depth': 45, 'ccp_alpha': 0.0, 'bootstrap': False}</td>\n",
       "      <td>-0.037894</td>\n",
       "      <td>0.002182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>{'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 'log2', 'max_depth': None, 'ccp_alpha': 0.001, 'bootstrap': False}</td>\n",
       "      <td>-0.038625</td>\n",
       "      <td>0.001823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 'sqrt', 'max_depth': 50, 'ccp_alpha': 0.0, 'bootstrap': False}</td>\n",
       "      <td>-0.038928</td>\n",
       "      <td>0.001546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                   params  mean_test_score  std_test_score\n",
       "130       {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': None, 'max_depth': 30, 'ccp_alpha': 0.0, 'bootstrap': False}        -0.036222        0.002755\n",
       "67         {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 1.0, 'max_depth': 50, 'ccp_alpha': 0.1, 'bootstrap': False}        -0.036372        0.002713\n",
       "8         {'n_estimators': 100, 'min_samples_split': 8, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.7, 'max_depth': 40, 'ccp_alpha': 0.01, 'bootstrap': False}        -0.036404        0.002429\n",
       "31     {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': None, 'max_depth': None, 'ccp_alpha': 0.01, 'bootstrap': False}        -0.036460        0.002638\n",
       "111      {'n_estimators': 300, 'min_samples_split': 4, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 1.0, 'max_depth': 20, 'ccp_alpha': 0.001, 'bootstrap': False}        -0.036617        0.002601\n",
       "163   {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'log2', 'max_depth': None, 'ccp_alpha': 0.1, 'bootstrap': False}        -0.037666        0.001694\n",
       "94      {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 35, 'ccp_alpha': 0.0, 'bootstrap': False}        -0.037889        0.001804\n",
       "4       {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 'log2', 'max_depth': 45, 'ccp_alpha': 0.0, 'bootstrap': False}        -0.037894        0.002182\n",
       "101  {'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 'log2', 'max_depth': None, 'ccp_alpha': 0.001, 'bootstrap': False}        -0.038625        0.001823\n",
       "138    {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 'sqrt', 'max_depth': 50, 'ccp_alpha': 0.0, 'bootstrap': False}        -0.038928        0.001546"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Top 10 parameters and scores\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results = results.sort_values(by='rank_test_score')\n",
    "results = results[['params', 'mean_test_score', 'std_test_score']]\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Best parameters found: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 0.5, 'max_depth': None, 'ccp_alpha': 0.0, 'bootstrap': False}\n",
      "Best cross-validation score: -0.036635515396364074\n",
      "Train set R² score: 1.0\n",
      "Train set MAPE: 0.00%\n",
      "Test set R² score: 0.9699415897119722\n",
      "Test set MAPE: 3.52%\n"
     ]
    }
   ],
   "source": [
    "X = df_OHE.drop('claim', axis=1)\n",
    "\n",
    "y = df_OHE['claim']\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model directly (no pipeline needed)\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=1)  # Enable parallel tree training\n",
    "\n",
    "# Expanded parameter grid with corrected max_features values\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "    'max_features': ['sqrt', 'log2', None, 0.5, 0.7, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.1],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.01, 0.1]  \n",
    "}\n",
    "\n",
    "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)  # Minimize error\n",
    "\n",
    "# Set up the RandomizedSearchCV\n",
    "random_search2 = RandomizedSearchCV(\n",
    "    estimator=rf,       # Directly use the RandomForestRegressor\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=200,         # Number of parameter settings sampled\n",
    "    cv=5,              # 5-fold cross-validation\n",
    "    n_jobs=18,         # Use all available cores for parallel search\n",
    "    random_state=42,   # For reproducibility\n",
    "    verbose=3,         # Show intermediate progress\n",
    "    scoring=mape_scorer,\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV on the training data\n",
    "random_search2.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the training set\n",
    "y_train_pred2 = random_search2.best_estimator_.predict(X_train)\n",
    "r2_train2 = r2_score(y_train, y_train_pred2)\n",
    "mape_train2 = mean_absolute_percentage_error(y_train, y_train_pred2) * 100  \n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred2 = random_search2.best_estimator_.predict(X_test)\n",
    "r2_test2 = r2_score(y_test, y_test_pred2)\n",
    "mape_test2 = mean_absolute_percentage_error(y_test, y_test_pred2) * 100  # Convert to percentage\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters found:\", random_search2.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search2.best_score_)\n",
    "print(\"Train set R² score:\", r2_train2)\n",
    "print(f\"Train set MAPE: {mape_train2:.2f}%\")\n",
    "print(\"Test set R² score:\", r2_test2)\n",
    "print(f\"Test set MAPE: {mape_test2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                 params  mean_test_score  std_test_score\n",
      "123    {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 0.5, 'max_depth': None, 'ccp_alpha': 0.0, 'bootstrap': False}        -0.036636        0.001211\n",
      "124    {'n_estimators': 200, 'min_samples_split': 6, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 0.7, 'max_depth': 25, 'ccp_alpha': 0.01, 'bootstrap': False}        -0.042270        0.004697\n",
      "14    {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 1.0, 'max_depth': 45, 'ccp_alpha': 0.001, 'bootstrap': False}        -0.043596        0.002551\n",
      "115      {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.1, 'max_features': 0.5, 'max_depth': 50, 'ccp_alpha': 0.1, 'bootstrap': False}        -0.043919        0.003078\n",
      "125    {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.01, 'max_features': 0.5, 'max_depth': 35, 'ccp_alpha': 0.01, 'bootstrap': False}        -0.044189        0.003364\n",
      "46     {'n_estimators': 300, 'min_samples_split': 6, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.1, 'max_features': 0.7, 'max_depth': 30, 'ccp_alpha': 0.001, 'bootstrap': False}        -0.045870        0.005829\n",
      "95    {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.5, 'max_depth': 25, 'ccp_alpha': 0.001, 'bootstrap': False}        -0.046628        0.003662\n",
      "68     {'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 0.5, 'max_depth': None, 'ccp_alpha': 0.0, 'bootstrap': False}        -0.046685        0.003166\n",
      "1      {'n_estimators': 500, 'min_samples_split': 6, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': None, 'max_depth': 50, 'ccp_alpha': 0.0, 'bootstrap': False}        -0.047203        0.005914\n",
      "128  {'n_estimators': 50, 'min_samples_split': 6, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': None, 'max_depth': None, 'ccp_alpha': 0.01, 'bootstrap': False}        -0.047283        0.005919\n"
     ]
    }
   ],
   "source": [
    "# Get Top 10 parameters and scores\n",
    "results = pd.DataFrame(random_search2.cv_results_)\n",
    "results = results.sort_values(by='rank_test_score')\n",
    "results = results[['params', 'mean_test_score', 'std_test_score']]\n",
    "print(results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. OHE does not really improve accuracy, LE seems sufficient for tree-based models.\n",
    "\n",
    "2. Number of estimators does not seem to big of an importance, therefore smaller models should work nearly as fine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
