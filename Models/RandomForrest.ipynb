{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Expand display options\n",
    "pd.set_option(\"display.max_rows\", None)  # Show all rows\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Do not truncate column text\n",
    "pd.set_option(\"display.expand_frame_repr\", False)  # Avoid line wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OHE = pd.read_csv('../DataSet/RegressionData/healthinsurance_OHE.csv')\n",
    "df_LE = pd.read_csv('../DataSet/RegressionData/healthinsurance_LE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Best parameters found: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'log2', 'max_depth': None, 'ccp_alpha': 0.1, 'bootstrap': False}\n",
      "Best cross-validation score: 0.9743136379207892\n",
      "Train set R² score: 0.999999999299343\n",
      "Train set MAPE: 0.00%\n",
      "Test set R² score: 0.972868862340848\n",
      "Test set MAPE: 3.64%\n"
     ]
    }
   ],
   "source": [
    "X = df_LE.drop('claim', axis=1)\n",
    "\n",
    "y = df_LE['claim']\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model directly (no pipeline needed)\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=1)  # Enable parallel tree training\n",
    "\n",
    "# Expanded parameter grid with corrected max_features values\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400],\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "    'max_features': ['sqrt', 'log2', None, 0.5, 0.7, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.1],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.01, 0.1]  \n",
    "}\n",
    "\n",
    "# Set up the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,       # Directly use the RandomForestRegressor\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=200,         # Number of parameter settings sampled\n",
    "    cv=5,              # 5-fold cross-validation\n",
    "    n_jobs=18,         # Use all available cores for parallel search\n",
    "    random_state=42,   # For reproducibility\n",
    "    verbose=3,         # Show intermediate progress\n",
    "    scoring='r2',\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV on the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the training set\n",
    "y_train_pred = random_search.best_estimator_.predict(X_train)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred) * 100  # Convert to\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = random_search.best_estimator_.predict(X_test)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred) * 100  # Convert to percentage\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "print(\"Train set R² score:\", r2_train)\n",
    "print(f\"Train set MAPE: {mape_train:.2f}%\")\n",
    "print(\"Test set R² score:\", r2_test)\n",
    "print(f\"Test set MAPE: {mape_test:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>{'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'log2', 'max_depth': None, 'ccp_alpha': 0.1, 'bootstrap': False}</td>\n",
       "      <td>0.974314</td>\n",
       "      <td>0.004506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 'log2', 'max_depth': 45, 'ccp_alpha': 0.0, 'bootstrap': False}</td>\n",
       "      <td>0.974181</td>\n",
       "      <td>0.004734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 35, 'ccp_alpha': 0.0, 'bootstrap': False}</td>\n",
       "      <td>0.974162</td>\n",
       "      <td>0.004542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 'sqrt', 'max_depth': 50, 'ccp_alpha': 0.0, 'bootstrap': False}</td>\n",
       "      <td>0.973704</td>\n",
       "      <td>0.004288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>{'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 'log2', 'max_depth': None, 'ccp_alpha': 0.001, 'bootstrap': False}</td>\n",
       "      <td>0.973403</td>\n",
       "      <td>0.004484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'n_estimators': 50, 'min_samples_split': 8, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 25, 'ccp_alpha': 0.01, 'bootstrap': False}</td>\n",
       "      <td>0.973358</td>\n",
       "      <td>0.003865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'n_estimators': 300, 'min_samples_split': 8, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 25, 'ccp_alpha': 0.0, 'bootstrap': False}</td>\n",
       "      <td>0.973157</td>\n",
       "      <td>0.004638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>{'n_estimators': 300, 'min_samples_split': 8, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 'sqrt', 'max_depth': 35, 'ccp_alpha': 0.001, 'bootstrap': False}</td>\n",
       "      <td>0.973151</td>\n",
       "      <td>0.004664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>{'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 'sqrt', 'max_depth': 35, 'ccp_alpha': 0.1, 'bootstrap': False}</td>\n",
       "      <td>0.973151</td>\n",
       "      <td>0.004459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 8, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 'sqrt', 'max_depth': 30, 'ccp_alpha': 0.1, 'bootstrap': False}</td>\n",
       "      <td>0.973136</td>\n",
       "      <td>0.004586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                   params  mean_test_score  std_test_score\n",
       "163   {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'log2', 'max_depth': None, 'ccp_alpha': 0.1, 'bootstrap': False}         0.974314        0.004506\n",
       "4       {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 'log2', 'max_depth': 45, 'ccp_alpha': 0.0, 'bootstrap': False}         0.974181        0.004734\n",
       "94      {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 35, 'ccp_alpha': 0.0, 'bootstrap': False}         0.974162        0.004542\n",
       "138    {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 'sqrt', 'max_depth': 50, 'ccp_alpha': 0.0, 'bootstrap': False}         0.973704        0.004288\n",
       "101  {'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 'log2', 'max_depth': None, 'ccp_alpha': 0.001, 'bootstrap': False}         0.973403        0.004484\n",
       "26      {'n_estimators': 50, 'min_samples_split': 8, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 25, 'ccp_alpha': 0.01, 'bootstrap': False}         0.973358        0.003865\n",
       "5       {'n_estimators': 300, 'min_samples_split': 8, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 25, 'ccp_alpha': 0.0, 'bootstrap': False}         0.973157        0.004638\n",
       "90   {'n_estimators': 300, 'min_samples_split': 8, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 'sqrt', 'max_depth': 35, 'ccp_alpha': 0.001, 'bootstrap': False}         0.973151        0.004664\n",
       "189     {'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 'sqrt', 'max_depth': 35, 'ccp_alpha': 0.1, 'bootstrap': False}         0.973151        0.004459\n",
       "154    {'n_estimators': 200, 'min_samples_split': 8, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_features': 'sqrt', 'max_depth': 30, 'ccp_alpha': 0.1, 'bootstrap': False}         0.973136        0.004586"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Top 10 parameters and scores\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results = results.sort_values(by='rank_test_score')\n",
    "results = results[['params', 'mean_test_score', 'std_test_score']]\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Best parameters found: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.1, 'max_features': 0.5, 'max_depth': 50, 'ccp_alpha': 0.1, 'bootstrap': False}\n",
      "Best cross-validation score: 0.970304804699498\n",
      "Train set R² score: 0.9960758330963112\n",
      "Train set MAPE: 1.77%\n",
      "Test set R² score: 0.9715220789108656\n",
      "Test set MAPE: 4.24%\n"
     ]
    }
   ],
   "source": [
    "X = df_OHE.drop('claim', axis=1)\n",
    "\n",
    "y = df_OHE['claim']\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model directly (no pipeline needed)\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=1)  # Enable parallel tree training\n",
    "\n",
    "# Expanded parameter grid with corrected max_features values\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "    'max_features': ['sqrt', 'log2', None, 0.5, 0.7, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.1],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.01, 0.1]  \n",
    "}\n",
    "\n",
    "# Set up the RandomizedSearchCV\n",
    "random_search2 = RandomizedSearchCV(\n",
    "    estimator=rf,       # Directly use the RandomForestRegressor\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=200,         # Number of parameter settings sampled\n",
    "    cv=5,              # 5-fold cross-validation\n",
    "    n_jobs=18,         # Use all available cores for parallel search\n",
    "    random_state=42,   # For reproducibility\n",
    "    verbose=3,         # Show intermediate progress\n",
    "    scoring='r2',\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV on the training data\n",
    "random_search2.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the training set\n",
    "y_train_pred2 = random_search2.best_estimator_.predict(X_train)\n",
    "r2_train2 = r2_score(y_train, y_train_pred2)\n",
    "mape_train2 = mean_absolute_percentage_error(y_train, y_train_pred2) * 100  \n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred2 = random_search2.best_estimator_.predict(X_test)\n",
    "r2_test2 = r2_score(y_test, y_test_pred2)\n",
    "mape_test2 = mean_absolute_percentage_error(y_test, y_test_pred2) * 100  # Convert to percentage\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters found:\", random_search2.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search2.best_score_)\n",
    "print(\"Train set R² score:\", r2_train2)\n",
    "print(f\"Train set MAPE: {mape_train2:.2f}%\")\n",
    "print(\"Test set R² score:\", r2_test2)\n",
    "print(f\"Test set MAPE: {mape_test2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                 params  mean_test_score  std_test_score\n",
      "115      {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.1, 'max_features': 0.5, 'max_depth': 50, 'ccp_alpha': 0.1, 'bootstrap': False}         0.970305        0.003605\n",
      "125    {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.01, 'max_features': 0.5, 'max_depth': 35, 'ccp_alpha': 0.01, 'bootstrap': False}         0.969917        0.003557\n",
      "123    {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 0.5, 'max_depth': None, 'ccp_alpha': 0.0, 'bootstrap': False}         0.969759        0.003817\n",
      "95    {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.5, 'max_depth': 25, 'ccp_alpha': 0.001, 'bootstrap': False}         0.968843        0.003968\n",
      "175     {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.5, 'max_depth': 20, 'ccp_alpha': 0.0, 'bootstrap': False}         0.968817        0.003963\n",
      "86     {'n_estimators': 100, 'min_samples_split': 8, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.1, 'max_features': 0.5, 'max_depth': 40, 'ccp_alpha': 0.001, 'bootstrap': False}         0.968536        0.003699\n",
      "157  {'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 30, 'ccp_alpha': 0.01, 'bootstrap': False}         0.968501        0.004286\n",
      "68     {'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 0.5, 'max_depth': None, 'ccp_alpha': 0.0, 'bootstrap': False}         0.968491        0.004217\n",
      "105     {'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 45, 'ccp_alpha': 0.0, 'bootstrap': True}         0.968408        0.003448\n",
      "77      {'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.1, 'max_features': 0.5, 'max_depth': 25, 'ccp_alpha': 0.1, 'bootstrap': False}         0.968264        0.003742\n"
     ]
    }
   ],
   "source": [
    "# Get Top 10 parameters and scores\n",
    "results = pd.DataFrame(random_search2.cv_results_)\n",
    "results = results.sort_values(by='rank_test_score')\n",
    "results = results[['params', 'mean_test_score', 'std_test_score']]\n",
    "print(results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. OHE does not really improve accuracy, LE seems sufficient for tree-based models.\n",
    "\n",
    "2. Number of estimators does not seem to big of an importance, therefore smaller models should work nearly as fine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
